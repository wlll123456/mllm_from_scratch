{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to /home/wjc/mllm/data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [03:13<00:00, 51252.25it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/wjc/mllm/data/mnist/MNIST/raw/train-images-idx3-ubyte.gz to /home/wjc/mllm/data/mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to /home/wjc/mllm/data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 96576.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/wjc/mllm/data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to /home/wjc/mllm/data/mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to /home/wjc/mllm/data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:15<00:00, 104042.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/wjc/mllm/data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to /home/wjc/mllm/data/mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to /home/wjc/mllm/data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 9361439.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/wjc/mllm/data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to /home/wjc/mllm/data/mnist/MNIST/raw\n",
      "\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PILToTensor 是 torchvision.transforms 模块中的一个类，用于将 PIL 图像转换为 PyTorch 的 Tensor\n",
    "# 输入图像的格式通常是 (H x W x C)，即高度、宽度和通道数，转换后的 Tensor 的格式是 (C x H x W)，即通道数、高度和宽度。\n",
    "# 这是因为 PyTorch 的 Tensor 在处理图像时，通道数是第一个维度。\n",
    "# Compose 是 torchvision.transforms 模块中的另一个类，用于将多个图像变换操作组合在一起。它允许你将一系列的图像转换操作按顺序应用于输入图像。\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms.v2 import PILToTensor,Compose\n",
    "import torchvision\n",
    "\n",
    "# 手写数字\n",
    "class MNIST(Dataset):\n",
    "    def __init__(self,is_train=True):\n",
    "        super().__init__()\n",
    "        self.ds=torchvision.datasets.MNIST('/home/wjc/mllm/data/mnist/',train=is_train,download=True)\n",
    "        # img_convert(img) 返回的张量的值范围在 [0, 255] 之间。\n",
    "        self.img_convert=Compose([\n",
    "            PILToTensor(),\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    \n",
    "    # 当通过 ds[0] 获取数据时，实际上是调用了 __getitem__ 方法。\n",
    "    def __getitem__(self,index):\n",
    "        img,label=self.ds[index]\n",
    "        # 返回归一化后的图像 Tensor 和标签。\n",
    "        return self.img_convert(img)/255.0,label\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    import matplotlib.pyplot as plt \n",
    "    \n",
    "    ds=MNIST()\n",
    "    img,label=ds[0]\n",
    "    print(label)\n",
    "    plt.imshow(img.permute(1,2,0))\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## img_encoder.py\n",
    "\n",
    "卷积核计算公式：https://www.cnblogs.com/zoubilin/p/17567156.\n",
    "\n",
    "ReLU（Rectified Linear Unit）激活函数为网络引入非线性.当我们在网络的每一层之后使用 ReLU（或其他非线性激活函数）时，网络的输出不再是输入的简单线性组合。通过这种方式，网络能够表示和学习到更复杂的函数和模式。\n",
    "\n",
    "    x = torch.randn(1, 1, 4, 4)  # 张量形状为 (1, 1, 4, 4)\n",
    "    x_flattened = x.view(x.size(0), -1)  # 将张量展平成 (1, 16)\n",
    "    print(x_flattened.shape)  # 输出: torch.Size([1, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "torch.Size([1, 8])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn \n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,stride):\n",
    "        super().__init__()\n",
    "        self.conv1=nn.Conv2d(in_channels=in_channels,out_channels=out_channels,kernel_size=3,padding=1,stride=stride)\n",
    "        self.bn1=nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.conv2=nn.Conv2d(in_channels=out_channels,out_channels=out_channels,kernel_size=3,padding=1,stride=1)\n",
    "        self.bn2=nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.conv3=nn.Conv2d(in_channels=in_channels,out_channels=out_channels,kernel_size=1,padding=0,stride=stride)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        y=F.relu(self.bn1(self.conv1(x)))\n",
    "        y=self.bn2(self.conv2(y))\n",
    "        z=self.conv3(x)\n",
    "        return F.relu(y+z)\n",
    "        \n",
    "\n",
    "class ImgEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.res_block1=ResidualBlock(in_channels=1,out_channels=16,stride=2) # (batch,16,14,14)\n",
    "        self.res_block2=ResidualBlock(in_channels=16,out_channels=4,stride=2) # (batch,4,7,7)\n",
    "        self.res_block3=ResidualBlock(in_channels=4,out_channels=1,stride=2) # (batch,1,4,4)\n",
    "        self.wi=nn.Linear(in_features=16,out_features=8)\n",
    "        self.ln=nn.LayerNorm(8)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.res_block1(x)\n",
    "        x=self.res_block2(x)\n",
    "        x=self.res_block3(x)\n",
    "        # print(x.size(0))\n",
    "        x=self.wi(x.view(x.size(0),-1))\n",
    "        x=self.ln(x)\n",
    "        return x\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    img_encoder=ImgEncoder()\n",
    "    # 因为输入的图片张量是 1x28x28 的，所以输入的通道数是 1。\n",
    "    out=img_encoder(torch.randn(1,1,28,28))\n",
    "    print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text_encoder.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 8])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn \n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 一共只有十个数字，所以 num_embeddings=10。\n",
    "        self.emb=nn.Embedding(num_embeddings=10,embedding_dim=16)\n",
    "        self.dense1=nn.Linear(in_features=16,out_features=64)\n",
    "        self.dense2=nn.Linear(in_features=64,out_features=16)\n",
    "        self.wt=nn.Linear(in_features=16,out_features=8)\n",
    "        self.ln=nn.LayerNorm(8)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.emb(x)\n",
    "        x=F.relu(self.dense1(x))\n",
    "        x=F.relu(self.dense2(x))\n",
    "        x=self.wt(x)\n",
    "        x=self.ln(x)\n",
    "        return x\n",
    "\n",
    "if __name__=='__main__':\n",
    "    text_encoder=TextEncoder()\n",
    "    x=torch.tensor([1,2,3,4,5,6,7,8,9,0])\n",
    "    y=text_encoder(x)\n",
    "    print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP.py\n",
    "return img_emb @ text_emb.T 这行代码中的 @ 运算符表示矩阵乘法运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "torch.Size([5, 5])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn \n",
    "import torch \n",
    "# from img_encoder import ImgEncoder\n",
    "# from text_encoder import TextEncoder\n",
    "\n",
    "class CLIP(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "        self.img_enc=ImgEncoder()\n",
    "        self.text_enc=TextEncoder()\n",
    "\n",
    "    def forward(self,img_x,text_x):\n",
    "        img_emb=self.img_enc(img_x)\n",
    "        text_emb=self.text_enc(text_x)\n",
    "        return img_emb@text_emb.T\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    clip=CLIP()\n",
    "    img_x=torch.randn(5,1,28,28)\n",
    "    text_x=torch.randint(0,10,(5,))\n",
    "    logits=clip(img_x,text_x)\n",
    "    print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.5934, -0.5750, -2.1580,  ...,  2.6855, -0.4067,  0.6574],\n",
       "          [-1.5243, -1.4248, -0.2115,  ...,  1.7521,  0.1227,  0.6983],\n",
       "          [ 1.0112,  0.8901, -0.1508,  ..., -1.2919,  0.4051, -2.7205],\n",
       "          ...,\n",
       "          [ 1.1697, -1.4691, -1.2244,  ...,  0.0223,  2.1281,  0.5019],\n",
       "          [ 0.2402, -1.5055, -1.3895,  ...,  1.5622, -0.4408,  0.4127],\n",
       "          [ 1.5139, -0.0595, -0.3256,  ..., -0.3279,  0.7978, -0.2185]]],\n",
       "\n",
       "\n",
       "        [[[ 0.8510, -0.5536, -0.7077,  ..., -0.9314,  1.1319, -0.0356],\n",
       "          [ 1.4177,  1.1410, -1.3114,  ..., -0.5541, -0.6159,  0.9672],\n",
       "          [-0.7171, -1.5755, -1.3565,  ..., -1.4452,  0.0304,  0.5849],\n",
       "          ...,\n",
       "          [-0.3193, -0.1108,  1.1973,  ..., -0.5583,  0.4985, -0.3087],\n",
       "          [-0.5008,  0.5747,  0.3786,  ...,  0.0545, -2.0439,  0.2778],\n",
       "          [ 0.5941, -0.4259, -0.0944,  ...,  0.1799, -0.1880, -0.0904]]],\n",
       "\n",
       "\n",
       "        [[[ 1.4839,  0.7221, -0.6917,  ..., -0.5925, -0.6376, -2.2693],\n",
       "          [ 0.2986, -1.0303,  0.5321,  ...,  1.5790, -0.9893,  0.6389],\n",
       "          [ 1.2459, -0.0572, -0.1210,  ..., -0.8457,  2.7942, -1.9121],\n",
       "          ...,\n",
       "          [-0.8728,  0.6558,  1.6275,  ..., -0.1027, -0.1273,  1.4058],\n",
       "          [-0.8909,  0.1568, -1.1414,  ..., -0.8247, -0.8497, -0.8631],\n",
       "          [ 0.4398,  2.2000,  1.3239,  ..., -0.3567,  0.2540, -0.3140]]],\n",
       "\n",
       "\n",
       "        [[[ 0.9684,  0.3770, -0.7810,  ..., -0.8255,  0.2580, -0.1499],\n",
       "          [-0.0970,  0.9489,  1.3783,  ..., -0.9812,  0.4020,  0.7426],\n",
       "          [ 0.0519,  0.0113,  0.8433,  ..., -1.0273,  0.9241, -0.2714],\n",
       "          ...,\n",
       "          [-0.4207,  0.0129, -0.4919,  ...,  0.7996,  1.6405,  0.8325],\n",
       "          [-1.2434,  1.6025, -0.4044,  ..., -0.3046,  0.3831, -1.0712],\n",
       "          [-0.9809, -1.0631, -0.5225,  ...,  0.2587, -0.5750,  1.0497]]],\n",
       "\n",
       "\n",
       "        [[[ 1.9121, -1.3489, -0.4183,  ..., -0.5694,  1.4186,  2.0700],\n",
       "          [ 0.1382,  0.8218, -0.0243,  ...,  1.3945,  0.7105,  1.5604],\n",
       "          [ 1.0615,  1.9092,  0.2958,  ...,  1.9209,  0.0145, -0.8261],\n",
       "          ...,\n",
       "          [ 1.8508, -0.1278, -0.2174,  ..., -1.7794, -1.1172,  0.0262],\n",
       "          [ 0.3869, -1.1925,  0.1462,  ..., -0.3964,  1.4034, -1.3196],\n",
       "          [-0.2476,  0.6501, -1.4747,  ..., -0.9684, -0.8949,  0.9273]]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 8, 7, 4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.8960, 5.9459, 5.8762, 3.0344, 4.1567],\n",
       "        [4.9482, 2.3083, 4.1040, 2.1235, 5.0371],\n",
       "        [5.1619, 6.9312, 7.6726, 4.1178, 6.4024],\n",
       "        [4.0675, 2.9430, 2.3627, 4.7666, 1.0974],\n",
       "        [2.6265, 4.9236, 4.9821, 2.3780, 2.8426]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from dataset import MNIST\n",
    "# from clip import CLIP\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import os \n",
    "\n",
    "DEVICE='cuda' if torch.cuda.is_available() else 'cpu'   # 设备\n",
    "\n",
    "dataset=MNIST() # 数据集\n",
    "\n",
    "model=CLIP().to(DEVICE) # 模型\n",
    "\n",
    "try:    # 加载模型\n",
    "    model.load_state_dict(torch.load('model.pth'))\n",
    "except:\n",
    "    pass \n",
    "\n",
    "optimzer=torch.optim.Adam(model.parameters(),lr=1e-3)   # 优化器\n",
    "\n",
    "'''\n",
    "    训练模型\n",
    "'''\n",
    "ITER_BATCH_COUNT=100000    # 迭代次数\n",
    "BATCH_SIZE=64   # 从batch内选出10个不一样的数字\n",
    "TARGET_COUNT=10 # 共10种数字\n",
    "\n",
    "dataloader=DataLoader(dataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=10,persistent_workers=True)    # 数据加载器\n",
    "\n",
    "for i in range(ITER_BATCH_COUNT):\n",
    "    while True:\n",
    "        imgs,labels=next(iter(dataloader))\n",
    "        if torch.unique(labels).shape[0]<TARGET_COUNT:  # 未覆盖10种数字\n",
    "            continue\n",
    "        # 挑选出10个数字\n",
    "        target=set()    \n",
    "        indexes=[]\n",
    "        for j in range(BATCH_SIZE):\n",
    "            if labels[j].item() in target:\n",
    "                continue \n",
    "            target.add(labels[j].item())\n",
    "            indexes.append(j)\n",
    "            if len(target)==TARGET_COUNT:\n",
    "                break\n",
    "        imgs=imgs[indexes]\n",
    "        labels=labels[indexes]\n",
    "        break\n",
    "\n",
    "    logits=model(imgs.to(DEVICE),labels.to(DEVICE))\n",
    "    \n",
    "    targets=torch.arange(0,TARGET_COUNT).to(DEVICE)\n",
    "    loss_i=F.cross_entropy(logits,targets)\n",
    "    loss_t=F.cross_entropy(logits.permute(1,0),targets)\n",
    "    loss=(loss_i+loss_t)/2\n",
    "    \n",
    "    optimzer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimzer.step()\n",
    "    if i%1000==0:\n",
    "        print('iter:{},loss:{}'.format(i,loss))\n",
    "        torch.save(model.state_dict(),'.model.pth')\n",
    "        os.replace('.model.pth','model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
