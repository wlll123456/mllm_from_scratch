{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram.py\n",
    "\n",
    "Bigram 模型是一种简单的语言模型，它基于前一个词（或字符）来预测下一个词（或字符）的概率。具体来说，在给定当前词或字符的情况下，模型通过学习当前字符与下一个字符之间的关系，来预测下一个可能出现的字符。\n",
    "\n",
    "Bigram 模型会学习如下的字符对：\n",
    "- \"h\" -> \"e\"\n",
    "- \"e\" -> \"l\"\n",
    "- \"l\" -> \"l\"\n",
    "- \"l\" -> \"o\"\n",
    "\n",
    "训练之后，模型根据前一个字符来预测下一个字符。例如，当输入字符是 \"h\" 时，模型会预测 \"e\" 为下一个字符。\n",
    "\n",
    "Bigram 模型可以扩展为更高阶的 N-gram 模型\n",
    "\n",
    "Bigram 模型：\n",
    "\n",
    "\"Bi\" 在这个词中表示 “2”，即每次考虑两个连续的词或字符来进行建模。Bigram 模型会根据前一个词来预测下一个词。\n",
    "\n",
    "N-gram 模型：\n",
    "\n",
    "\"N\" 在这个词中表示 任意数，即 N 可以是 1、2、3 等等。N-gram 模型表示每次考虑 N 个连续的词或字符来进行建模。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入库\n",
    "\n",
    "导入了必要的PyTorch模块，包括神经网络模块nn和一些功能性函数F。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 超参数设置\n",
    "\n",
    "批量大小（batch_size）、上下文长度（block_size）、训练的最大迭代次数（max_iters）、评估间隔（eval_interval）、学习率（learning_rate）、设备（CPU或GPU），以及评估时的迭代次数（eval_iters）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "block_size = 8\n",
    "max_iters = 3000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据准备\n",
    "\n",
    "生成字符集的映射表（stoi 和 itos），用于将字符和整数索引相互转换。\n",
    "\n",
    "把字符转为int型，为了让计算机理解；计算机输出时，再转为字符，让人类理解\n",
    "\n",
    "stoi,itos可以理解为字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./ng-video-lecture/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练和验证数据集的拆分\n",
    "\n",
    "数据集分为训练集和验证集，训练集为前90%的数据，验证集为后10%的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据加载函数\n",
    "示例\n",
    "\n",
    "block_size = 3\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "data = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "ix = [2, 4]\n",
    "\n",
    "x = [[2, 3, 4],[4, 5, 6]]\n",
    "\n",
    "y = [[3, 4, 5],[5，6, 7]]\n",
    "\n",
    "y中与x相对应位置的值，为x应该预测的值，前面预测后面的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从训练集或者验证集取数据\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    # 把数据放到GPU运行\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.randint：生成在指定范围内均匀分布的随机整数张量。\n",
    "\n",
    "torch.randint(low, high, size, out=None, dtype=torch.int64, layout=torch.strided, device=None, requires_grad=False)\n",
    "\n",
    "    low：生成随机整数的下界（包含该值）。\n",
    "    high：生成随机整数的上界（不包含该值）。\n",
    "    size：生成的张量的形状，通常是一个元组，表示每个维度的大小。\n",
    "    out（可选）：输出的张量。\n",
    "    dtype（可选）：张量的类型，默认是 torch.int64。\n",
    "    layout（可选）：张量的布局，默认是 torch.strided。\n",
    "    device（可选）：张量所在的设备（CPU 或 GPU）。\n",
    "    requires_grad（可选）：如果为 True，则会为生成的张量记录梯度。\n",
    "\n",
    "torch.stack 是 PyTorch 中的一个函数，用于沿着一个新维度将一组张量拼接在一起。与 torch.cat 不同的是，torch.stack 会在指定的维度上添加一个新的维度，并在该维度上堆叠输入的张量。\n",
    "\n",
    "torch.stack(tensors, dim=0, out=None)\n",
    "\n",
    "    tensors：一个可迭代的张量序列（如列表、元组等）。这些张量的形状必须相同。\n",
    "    dim（可选）：要在其上插入新维度的位置。默认是 0，表示在第一个维度（批次维度）上堆叠张量。\n",
    "    out（可选）：输出张量。如果指定了这个参数，那么结果会直接写入这个张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1,  2,  3,  4],\n",
      "         [ 5,  6,  7,  8],\n",
      "         [ 9, 10, 11, 12]],\n",
      "\n",
      "        [[13, 14, 15, 16],\n",
      "         [17, 18, 19, 20],\n",
      "         [21, 22, 23, 24]]])\n",
      "tensor([[[ 1,  2,  3,  4],\n",
      "         [13, 14, 15, 16]],\n",
      "\n",
      "        [[ 5,  6,  7,  8],\n",
      "         [17, 18, 19, 20]],\n",
      "\n",
      "        [[ 9, 10, 11, 12],\n",
      "         [21, 22, 23, 24]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建两个形状为 (3, 4) 的张量\n",
    "a = torch.tensor([[1, 2, 3, 4],\n",
    "                  [5, 6, 7, 8],\n",
    "                  [9, 10, 11, 12]])\n",
    "\n",
    "b = torch.tensor([[13, 14, 15, 16],\n",
    "                  [17, 18, 19, 20],\n",
    "                  [21, 22, 23, 24]])\n",
    "\n",
    "# 在第0个维度上堆叠\n",
    "stacked_tensor = torch.stack([a, b], dim=0)\n",
    "print(stacked_tensor)\n",
    "stacked_tensor = torch.stack([a, b], dim=1)\n",
    "print(stacked_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 估计模型损失\n",
    "定义了一个用于估计训练和验证集损失的函数estimate_loss，在评估模式下对模型进行评估，然后返回平均损失。\n",
    "\n",
    "@torch.no_grad() 是一个上下文管理器，用于临时禁止梯度计算。\n",
    "在某些情况下，如模型评估或推理时，我们不需要计算梯度，因为我们只需要前向传播来获得预测结果。使用 @torch.no_grad() 可以节省内存和计算资源。\n",
    "\n",
    "model.eval() 将模型设置为评估模式；model.train() 将模型设置为训练模式（training mode）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    \n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义基于 Bigram 的语言模型\n",
    "\n",
    "nn.Embedding 层的主要功能是将每个输入的整数索引映射到一个高维向量。\n",
    "\n",
    "    # 定义一个嵌入层，词汇表大小为 10,000，嵌入向量的维度为 300\n",
    "    embedding = nn.Embedding(num_embeddings=10000, embedding_dim=300)\n",
    "\n",
    "    # 输入一个批量大小为 2，序列长度为 4 的索引张量\n",
    "    input_indices = torch.tensor([[1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "\n",
    "    # 通过嵌入层将索引映射到嵌入向量\n",
    "    output = embedding(input_indices)\n",
    "\n",
    "    print(output.shape)  # 输出形状为 (2, 4, 300)    \n",
    "\n",
    "\n",
    "view 进行形状调整\n",
    "    # 假设 logits 是模型的输出，形状为 (B, T, C)\n",
    "    B, T, C = 2, 3, 4\n",
    "    logits = torch.randn(B, T, C)\n",
    "\n",
    "    # 假设 targets 是目标标签，形状为 (B, T)\n",
    "    targets = torch.randint(0, C, (B, T))\n",
    "\n",
    "    # 查看原始形状\n",
    "    print(\"Original logits shape:\", logits.shape)  # Output: (2, 3, 4)\n",
    "    print(\"Original targets shape:\", targets.shape)  # Output: (2, 3)\n",
    "\n",
    "    # 调整形状\n",
    "    logits = logits.view(B*T, C)\n",
    "    targets = targets.view(B*T)\n",
    "\n",
    "    # 查看调整后的形状\n",
    "    print(\"Reshaped logits shape:\", logits.shape)  # Output: (6, 4)\n",
    "    print(\"Reshaped targets shape:\", targets.shape)  # Output: (6)\n",
    "\n",
    "交叉熵（Cross-Entropy）是用来衡量两个概率分布之间差异的损失函数。它通常用于分类任务，特别是在多分类任务中非常常见。\n",
    "    https://blog.csdn.net/weixin_44211968/article/details/123906631\n",
    "\n",
    "torch.multinomial 可以让你根据模型预测的概率分布，从候选类别中抽取一个。\n",
    "\n",
    "\n",
    "\n",
    "generate 函数运行过程\n",
    "\n",
    "idx = torch.tensor([[1, 2, 3]])  # 假设初始输入为序列 [1, 2, 3]\n",
    "\n",
    "运行 generate 函数5步，生成新token\n",
    "generated_idx = model.generate(idx, max_new_tokens=5)\n",
    "\n",
    "第一步：\n",
    "\n",
    "输入序列 idx 为 [1, 2, 3]。\n",
    "模型根据当前序列生成 logits，并且我们只关注最后一个时间步的 logits。\n",
    "通过 softmax 转换为概率分布，从中采样得到一个新 token，假设采样结果为 4。\n",
    "将 4 追加到序列，得到 [1, 2, 3, 4]。\n",
    "第二步：\n",
    "\n",
    "当前序列 idx 为 [1, 2, 3, 4]。\n",
    "模型再次生成 logits，并采样一个新 token，假设采样结果为 6。\n",
    "将 6 追加到序列，得到 [1, 2, 3, 4, 6]。\n",
    "第三步：\n",
    "\n",
    "当前序列 idx 为 [1, 2, 3, 4, 6]。\n",
    "模型生成 logits，采样得到新 token，假设为 2。\n",
    "将 2 追加到序列，得到 [1, 2, 3, 4, 6, 2]。\n",
    "第四步：\n",
    "\n",
    "当前序列 idx 为 [1, 2, 3, 4, 6, 2]。\n",
    "模型生成 logits，采样得到新 token，假设为 9。\n",
    "将 9 追加到序列，得到 [1, 2, 3, 4, 6, 2, 9]。\n",
    "第五步：\n",
    "\n",
    "当前序列 idx 为 [1, 2, 3, 4, 6, 2, 9]。\n",
    "模型生成 logits，采样得到新 token，假设为 7。\n",
    "将 7 追加到序列，得到 [1, 2, 3, 4, 6, 2, 9, 7]。\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.optim.AdamW 是 PyTorch 中的一个优化器，用于调整神经网络模型的参数以最小化损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizer.step() 是 PyTorch 中的一个方法，用于执行优化器的参数更新步骤。它基于当前计算出的梯度，按照优化算法的规则更新模型的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.6095, val loss 4.5949\n",
      "step 300: train loss 2.7809, val loss 2.8023\n",
      "step 600: train loss 2.5394, val loss 2.5500\n",
      "step 900: train loss 2.5028, val loss 2.5226\n",
      "step 1200: train loss 2.4807, val loss 2.5057\n",
      "step 1500: train loss 2.4731, val loss 2.4894\n",
      "step 1800: train loss 2.4700, val loss 2.4961\n",
      "step 2100: train loss 2.4621, val loss 2.4883\n",
      "step 2400: train loss 2.4611, val loss 2.4919\n",
      "step 2700: train loss 2.4565, val loss 2.4870\n",
      "\n",
      "Ourt weffouts I ss,\n",
      "\n",
      "INoty my t phthoun.\n",
      "end t win?\n",
      "\n",
      "Tor y no woinste gr pithBuinores:\n",
      "\n",
      "GEde hero wiungie'RETons!\n",
      "Hathe! t no thanesh.\n",
      "Thy manen t ind int' thie NUStegignathanangth veicemothas ghithemomes ie. trt litat,\n",
      "INRK:\n",
      "\n",
      "Bll accad.\n",
      "GLUExivesw,-n geavin m ththe'e'?\n",
      "\n",
      "A:\n",
      "\n",
      "Fio my ntheare ds y\n",
      "hawon;\n",
      "NI o CHe,\n",
      "USeremed gse d he\n",
      "A ulllshe the annow, wiche-NV: ge t k; pr'get pll--be by; iA ts cthe 'ves ly,\n",
      "Kithe byo aie nofd n is toupome,Tenthist!\n",
      "\n",
      "\n",
      "ARou.\n",
      "On h!\n",
      "\n",
      "Hat,\n",
      "Iffughomabbeeke nd ber.\n",
      "A:\n",
      "Ma\n"
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
